# lexer

## Lexers written in some languages for study purposes

## Key Concepts

### Token

- A token is a pair of token name and a optional attribute value. The **name** is a abstract symbol (sign) representing a lexical unit (sequence of input characters that denote an identifier) that are processed by the parser.


### Pattern

- A pattern is a description of the form a lexeme or a token may take. It might be a sequence of characters that form a keyword or a more complex structure matched by one or more strings.

### Lexeme

- A lexeme is a sequence of characters (or tokens) in the source code that matches the expected token pattern and is identified by the lexical analyzer as a instance of that token.

### Abstract Syntax Tree (AST)

- An AST is a data structure used to represent the structure of a text (often source code). Each of the nodes denotes a construct or structure that occurs in the text. It is called **abstract** because it does not represent every detail but it rather just contains the structural details or metadata used by the parser, like start/end position, token value and token name.

### Parse Tree (Concrete Syntax Trees)

- Parse trees or concrete syntax trees are typically built by the parser during the text/source code translation or compiling process. Once done, the contextual/semantic information is added to the AST in order to include features like type checking if a variable is used after its declaration. Some of the formal metalanguages are **extended Backus-Naur form (EBNF)**, **Wirth Syntax** and **augmented Backus-Naur form (ABNF)**. They are used to define formal metalanguages for computer languages.

### Metasyntax

- Metalanguages possess their own metasyntax, which is composed by **terminal symbols**, **nonterminal symbols** and **metasymbols**.
    - Terminal symbols are words/tokens is stand-alone structure, which means they appear in the final output, but cannot be broken down by grammar rules. They form the vocabulary of the target language (e.g: + if () - , else). Since they form the vocabulary, they are the end product of the derivation process, meaning they cannot be derived any further.
    - Nonterminal symbols, unlike terminal symbols, cannot appear in the formal language grammar. This means they are abstract placeholders that eventually break down into terminal symbols. They define syntactic rules to build valid sentences in the target laguage (e.g: Statement, Term, Number, Program). They can be derived following a process of Program -> Statement -> function(Term) -> function(number).
    - Metasymbols are symbols used to describe the syntax, structure and metadata of a language. They define boundaries, enclosings, denote optional/mandatory elements (e.g: " ;{} []). This means they have special meanings to a computer program, such as a compiler/interpreter/regex engine. Since they hold special meaning, they often might need to be escaped in order to strip them of their original meaning as needed.

### The Process: Input -> Lexer -> Parser

- The lexer starts working by taking all the input symbols and characters and converting them into tokens and/or lexemes, depending if they are one character (token) or a sequence of characters (lexeme) according to a set of defined rules that serve a specific purpose (e.g: a language compiler or a JSON parser).
- Typically, the conversions made by the lexer holds data like **token value**, the **token name**, the **start position**, the **end position**. 
- Usually, the literature does not use often the term **lexeme** since the token already holds the start and end position, blending semantically these two concepts.
- In that sense, a parser does not work directly with the input but rather gets a list of tokens generated by a lexer, in which each token is already identified and categorized.
- After taking the categorized tokens provided by the lexer, the parser analyzes them sequentially to verify if they follow the expected syntax. The start/end position might be used for debugging and error throwing.
- If the syntax is right, the token list is transformed into a abstract syntax tree (AST) which shows the structure of the source code without including nonessential punctuation and delimiters (e.g: braces, semicolons, parentheses, etc).
- After building up the AST, the source code is then analyzed in order to identify the syntactic structure of the program, generating a **parse tree** that replaces the linear structure of tokens with a tree structure according to the rules pertaining the formal grammar of the target language. - With the parse tree indicating the syntactic relation between the tokens, the **semantic analysis** adds semantic information to the parse tree and builds the **symbol table**. In this phase, processes like type, variable and scope checking are executed. It logically preceeds the code generation phase, being able to be done in one or more iterations.










